{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea5fd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "#리뷰 데이터 불러 옴\n",
    "reviews_train = load_files(\"data/aclImdb/train\")\n",
    "# 트레인용 텍스트를 나눔\n",
    "text_train, y_train = reviews_train.data[:1000], reviews_train.target[:1000]\n",
    "# 간단한 텍스트를 지워야 한다면, list 컨프렌시브를 써서 지워야할 텍스트를 지우는 것이 가장 빠름\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2534742",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9a7eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2x12 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 15 stored elements in Compressed Sparse Row format>\n",
      "[[0 0 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\n",
    "    \"The fool doth think h is wise,\",\n",
    "    \"But the wise man knows himself to be a fool\"\n",
    "]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "\n",
    "vect.vocabulary_ # 단어의 카운터를 세는 함수\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(repr(bag_of_words)) # 만든 행렬 크키\n",
    "print(bag_of_words.toarray()) # 만든 행렬 형태\n",
    "\n",
    "# 13개의 단어를 사용했고, 그 단어들 중 이 문장이 사용된 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8bb476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_features=1000, max_df=0.5)# 텍스트 데이터는 특징 여러게 있어야함. \n",
    "# 다른 모델은 특징을 줄이나 여기서는 다름 / 특히 50% 이상 나오는 단어는 버림\n",
    "\n",
    "X = vec.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b0f8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=25, random_state=42)\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9557f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽:0 주요 단어: ['people' 'would' 'from' 'by' 'only' 'about' 'just' 'those' 'get' 'there']\n",
      "토픽:1 주요 단어: ['my' 'like' 'they' 'love' 'time' 'just' 'when' 'more' 'good' 'very']\n",
      "토픽:2 주요 단어: ['very' 'story' 'by' 'its' 'some' 'great' 'well' 'more' 'characters'\n",
      " 'there']\n",
      "토픽:3 주요 단어: ['show' 'episode' 'series' 'star' 'he' 'his' 'when' 'luke' 'from' 'my']\n",
      "토픽:4 주요 단어: ['from' 'there' 'by' 'even' 'some' 'than' 'or' 'out' 'like' 'plot']\n",
      "토픽:5 주요 단어: ['who' 'they' 'his' 'he' 'her' 'she' 'out' 'by' 'there' 'we']\n",
      "토픽:6 주요 단어: ['so' 'if' 'they' 'like' 'there' 'my' 'just' 'bad' 'can' 'me']\n",
      "토픽:7 주요 단어: ['he' 'his' 'him' 'from' 'which' 'by' 'has' 'when' 'had' 'man']\n",
      "토픽:8 주요 단어: ['her' 'she' 'he' 'his' 'who' 'by' 'good' 'has' 'does' 'role']\n",
      "토픽:9 주요 단어: ['we' 'his' 'so' 'he' 'or' 'what' 'can' 'by' 'people' 'about']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feat_names = np.array(vec.get_feature_names_out())\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_feats = feat_names[sorting[topic_idx, :10]]\n",
    "    print(f\"토픽:{topic_idx} 주요 단어: {top_feats}\")\n",
    "    \n",
    "    # 0번 째가 좋은 것이 아니라, 이 토픽 중 중요한 단어를 내가 찾아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1500c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 100개 / max가 25가 좋은지 30이 좋은지 모른다. 데이터 나오는 것보고 선택\n",
    "lda00 = LatentDirichletAllocation(n_components=100, learning_method=\"batch\", max_iter=25, random_state=42)\n",
    "document_topics100 = lda00.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "130a1eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 0james family',\n",
       " ' 1they very',\n",
       " ' 2if his',\n",
       " ' 3from after',\n",
       " ' 4many out',\n",
       " ' 5by there',\n",
       " ' 6story great',\n",
       " ' 7he city',\n",
       " ' 8great me',\n",
       " ' 9us so',\n",
       " '10were such',\n",
       " '11they series',\n",
       " '12she her',\n",
       " '13original its',\n",
       " '14game very',\n",
       " '15action his',\n",
       " '16he his',\n",
       " '17movies ok',\n",
       " '18best story',\n",
       " '19people show',\n",
       " '20short no',\n",
       " '21like my',\n",
       " '22his he',\n",
       " '23like would',\n",
       " '24or like',\n",
       " '25we there',\n",
       " '26by we',\n",
       " '27cheesy out',\n",
       " '28she her',\n",
       " '29some just',\n",
       " '30de his',\n",
       " '31movies stupid',\n",
       " '32spirit they',\n",
       " '33there he',\n",
       " '34part can',\n",
       " '35than just',\n",
       " '36lincoln would',\n",
       " '37about so',\n",
       " '38fi sci',\n",
       " '3910 zombies',\n",
       " '40from just',\n",
       " '41out my',\n",
       " '42her she',\n",
       " '43more or',\n",
       " '44even some',\n",
       " '45from like',\n",
       " '46from through',\n",
       " '47songs band',\n",
       " '48my his',\n",
       " '49which has',\n",
       " '50his he',\n",
       " '51my just',\n",
       " '52car there',\n",
       " '53art even',\n",
       " '54video name',\n",
       " '55if your',\n",
       " '56its like',\n",
       " '57from by',\n",
       " '58his ll',\n",
       " '59or time',\n",
       " '60he when',\n",
       " '61so his',\n",
       " '62great well',\n",
       " '63like there',\n",
       " '64his like',\n",
       " '65bad ever',\n",
       " '66they like',\n",
       " '67her really',\n",
       " '68steve his',\n",
       " '69by who',\n",
       " '70would so',\n",
       " '71has single',\n",
       " '72he his',\n",
       " '73like they',\n",
       " '74tv written',\n",
       " '75my who',\n",
       " '76his he',\n",
       " '77his he',\n",
       " '78some or',\n",
       " '79no should',\n",
       " '80story which',\n",
       " '81show they',\n",
       " '82acting watching',\n",
       " '83other each',\n",
       " '84original only',\n",
       " '85there no',\n",
       " '86blood he',\n",
       " '87they if',\n",
       " '88bad good',\n",
       " '89book read',\n",
       " '90seen ve',\n",
       " '91good horror',\n",
       " '92his by',\n",
       " '93his michael',\n",
       " '94novel no',\n",
       " '95well very',\n",
       " '96also role',\n",
       " '97killed what',\n",
       " '98better much',\n",
       " '99they he']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorting = np.argsort(lda00.components_, axis=1)[:, ::-1]\n",
    "feat_names = np.array(vec.get_feature_names_out())\n",
    "\n",
    "topic_names = [\n",
    "    f\"{i:>2}\" + \" \".join(words)\n",
    "    for i , words in enumerate(feat_names[sorting[:, :2]])\n",
    "]\n",
    "\n",
    "topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e154848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.755 0.78  0.795 0.78  0.81 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X, y_train, n_jobs=-1)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
