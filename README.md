```markdown
# 머신러닝 연습

## 설정

- python >= 3.9
- 기타 라이브러리

```python
pip install scikit-learn
pip install jupyterlab
pip install seaborn
pip install pandas

# 📘 머신러닝 기본 개념 정리

## 1. 머신러닝이란?

> **특성(X)의 패턴을 학습하여 예측값(y^)을 도출하는 모델을 만드는 과정**

예:  
특성 X가 10개인 데이터가 있다면,  
모델은 이를 이용해 다음과 같은 식을 학습한다:

y^ = w0·x0 + w1·x1 + ... + w9·x9

---

## 2. 머신러닝의 종류

### ✅ 지도 학습 (Supervised Learning)
- 입력(X)과 정답(y)을 모두 제공
- 목적: 주어진 데이터를 통해 **입력과 출력 간의 관계** 학습

#### 🧮 지도 학습의 유형
| 유형 | 설명 | 예시 |
|------|------|------|
| 회귀 (Regression) | y가 연속적인 수치 | 집값, 온도 |
| 분류 (Classification) | y가 범주형 | 고양이/개, 합격/불합격 |

---

## 3. 선형 모델의 기본 구조

### ▶️ 선형 회귀 식

y^ = w·x + b


- `w`: 가중치 (weight, 기울기)
- `x`: 특성값
- `b`: 절편
- `y^`: 예측값

### ▶️ 선형 분류 식 (예: 로지스틱 회귀)

y^ = sigmoid(w·x + b)


---

## 4. 비용 함수 (Cost Function)

> 모델이 얼마나 틀렸는지 수치화하는 함수

### ▶️ 예: 평균 제곱 오차 (MSE)

Cost = (1/n) ∑(yᵢ - yᵢ^)²


- 이 값을 **최소화하는 방향**으로 학습함

---

## 5. 규제(Regularization): 과대적합 방지를 위한 제어장치

| 항목 | 설명 |
|------|------|
| 과대적합 | 모델이 훈련 데이터에 너무 과하게 맞춤 → 새로운 데이터에 약함 |
| 과소적합 | 모델이 너무 단순해서 데이터 패턴을 잘 못 잡음 |

---

## 6. 리지 회귀 (Ridge Regression)

> L2 규제를 추가하여 **가중치(w)를 너무 크게 만드는 걸 방지**

### ▶️ 리지 회귀의 비용 함수

Cost = ∑(yᵢ - yᵢ^)² + α × ∑(wᵢ²)


- `α`: **규제 계수 (regularization strength)**
- `∑(wᵢ²)`: L2 규제 항 (가중치 제곱합)
- `α`가 클수록 **w들을 0에 가깝게 줄이려 함**

---

## 7. L2 규제의 의미와 w 값의 변화

| α 값 | 규제 세기 | w에 대한 영향 | 결과 |
|------|-----------|----------------|--------|
| 작음 (≈ 0) | 규제 거의 없음 | w 커질 수 있음 | 예측에 집중 → 과대적합 위험 |
| 큼 (≫ 1) | 규제 강함 | w 작게 유지됨 | 모델 단순화 → 과소적합 위험 |

> 머신러닝 모델은 항상 **전체 비용을 줄이기 위해 w를 조정**한다.  
> `α`가 클수록 **w 값을 줄이는 쪽**으로 학습하게 된다.

### 🔍 질문:
왜 α가 작으면 w를 크게 하고, α가 크면 w를 작게 하려고 할까?

✅ “최적화 대상이 손실 함수 전체이기 때문”
머신러닝 모델은 항상 비용 함수 전체를 줄이려고(최소화) 함.
이때 α가 커지면 규제 항의 중요도가 올라가고, 반대로 작으면 규제 항은 거의 무시해도 되는 수준이 됨.

🔁 예시
✅ 1. α가 작을 때 (예: 0.0001)

비용 = 예측오차 + α * ∑(w²) ≈ 예측오차

규제 항의 영향력이 거의 없으니까, 모델은 오차를 줄이는 데 집중함 → 예측 오차 줄이려면 w를 막 키울 수도 있어

👉 이러면 w 값 커짐 → 과대적합 가능성 증가

✅ 2. α가 클 때 (예: 10)
비용 = 예측오차 + 10 * ∑(w²)
이제 규제 항이 너무 커서 무시 못 함

모델이 오차를 조금 줄이더라도, w가 크면 벌점이 커지니까

👉 w를 작게 유지하는 쪽으로 최적화함 → 과소적합 가능성도 있음



---

## 8. 용어 정리

| 용어 | 의미 |
|------|------|
| w | 가중치 (특성에 곱해지는 계수) |
| y^ | 예측값 |
| α (또는 lambda) | 규제 계수 (벌점의 무게) |
| L2 | 가중치 제곱합 = ∑(wᵢ²) |
| L2 규제 항 | α × ∑(wᵢ²) |
| 비용 함수 | 예측오차 + 규제항 |

---

## ✅ 한 줄 요약

> 머신러닝 모델은 예측 오차와 규제 항의 균형을 맞춰 가중치 `w`를 조정한다.  
> `α`가 클수록 `w`는 작아지고, 모델은 단순해지지만 **과소적합의 위험**이 생긴다.  
> 반대로 `α`가 작으면 모델은 더 정확하게 학습하려 하며, **과대적합의 위험**이 있다.
